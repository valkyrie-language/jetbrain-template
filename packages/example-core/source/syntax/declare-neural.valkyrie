neural LinearLayer {
    forward() {

    }
}
neural ReluLayer {
    forward() {

    }
}
neural SigmoidLayer {
    forward() {

    }
}
neural TanhLayer {
    forward() {

    }
}
neural ConvolutionLayer1D {
    weight: NDArray,
    bias: NDArray?,
    padding: usize,
    output_padding: usize,
    stride: usize,
    dilation: usize,
    groups: usize,
}
neural ConvolutionLayer2D {
    weight: NDArray,
    bias: NDArray?,
    padding: usize,
    stride: usize,
    dilation: usize,
    groups: usize,
}
neural ConvolutionLayer2DTranspose {
    weight: NDArray,
    bias: NDArray?,
    padding: usize,
    stride: usize,
    dilation: usize,
    groups: usize,
}


⍝ function neural network
micro relu() -> ReluLayer {

}
micro selu() -> SeluLayer {

}
micro tanh() -> TanhLayer {

}
micro conv2d() -> ConvolutionLayer2D {
    new ConvolutionLayer2D {

    }
}
micro conv2dt() -> ConvolutionLayer2DTranspose {
    new ConvolutionLayer2DTranspose {

    }
}

new conv2d() {
    bias = a
}

neural GroupNorm {
    weight: NDArray,
    bias: NDArray,
    epsilon: f64,
    channels: usize,
    groups: usize,
}
neural BatchNorm {
    weight: NDArray,
    bias: NDArray,
    epsilon: f64,
    remove_mean: bool,
    momentum: f64,
}

micro batch_norm(epsilon: f64 = 1×*-6) -> BatchNorm {
    new BatchNorm {
        epsilon = epsilon
    }
    EinsteinLayer("ij,jk")
    ReshapeLayer("nchw>n(chw)")
    ReshapeLayer("N C H W -> N H W C")
    ReduceLayer("B C H W -> B H W", mean)
    RepeatLayer("N C H W -> (repeat N) C H W", repeat = 4)
    ⍝ # Flatten spatial dimensions
    ReshapeLayer("N C H W -> N(C H W)")
}


⍝ einstein'...,...'(3, c)
#compiler
textual macro einstein(text: UTF8Text) -> AnyFunction {

}

⍝ einstein('...ii->...i', args)
functional macro `einstein`(..args): NDArray {

}








einstein('I I -> I', a)

einsum
flattened = rearrange(tensor, 'b c h w -> b (c h w)')


reduce(tensor, 'b c h w -> b h w', 'mean')
rearranged = rearrange(tensor, 'b c h w -> b h w c')
